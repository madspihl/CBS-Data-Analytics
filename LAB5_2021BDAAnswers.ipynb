{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 05 Text Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as df\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Movie Reviews\n",
    "We used the following commands to download and load the movie review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: you only need to execute this cell once!\n",
    "#!wget -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz -P data\n",
    "#!tar xzf data/aclImdb_v1.tar.gz -C data\n",
    "#!rm -r data/aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of text_train: <class 'list'>\n",
      "length of text_train: 25000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "# NOTE -- this takes a few minutes!\n",
    "reviews_train = load_files(\"data/aclImdb/train/\")\n",
    "# load_files returns a bunch, containing training texts and training labels\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "print(\"type of text_train: {}\".format(type(text_train)))\n",
    "print(\"length of text_train: {}\".format(len(text_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(text_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in test data: 25000\n",
      "Samples per class (test): [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "reviews_test = load_files(\"data/aclImdb/test/\")\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "print(\"Number of documents in test data: {}\".format(len(text_test)))\n",
    "print(\"Samples per class (test): {}\".format(np.bincount(y_test)))\n",
    "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 0\n",
    "\n",
    "Use CountVectorizer to transform the text_data, by completing the following code. Assign the transformed data to X_data. Note that we use <b>min_df</b> of 10 and <b>max_df</b> of .5. These settings help keep things from getting too slow. You can experiment with different values -- higher minimums and lower maximums can speed things up, potentially with a reduction in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "<25000x18497 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 2848822 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=10, max_df=.5)\n",
    "vect = vect.fit(text_train )\n",
    "X_train = vect.transform(text_train)\n",
    "X_test = vect.transform(text_test)\n",
    "\n",
    "print(\"X_train:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Fit a LogisticRegression to X_train and y_train. Use the optional parameter C=.1\n",
    "Print the training and test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielhardt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.967\n",
      "Test score: 0.88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(C=.1).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test score: {:.2f}\".format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "Make a MultinomialNB classifier for X_train and y_train. Print training and test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.876\n",
      "Test score: 0.83\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nbModel = MultinomialNB().fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(nbModel.score(X_train, y_train)))\n",
    "print(\"Test score: {:.2f}\".format(nbModel.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Try changing countVectorizer so that it constructs unigrams, bigrams and trigrams. You do this by setting the parameter ngram_range = (1,3), and create a new vectorizer. Now transform text_train and text_test, and report new train and test results for the two models you built above.\n",
    "(It might be convenient to use the function <b>applyModel</b>, defined below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(min_df=10, max_df=.5,ngram_range=(1,3))\n",
    "vect = vect.fit(text_train )\n",
    "X_train = vect.transform(text_train)\n",
    "X_test = vect.transform(text_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyModel(model,name,X_train, y_train, X_test, y_test):\n",
    "    m = model.fit(X_train,y_train)\n",
    "    print(\"{} Training score: {:.3f}\".format(name, m.score(X_train, y_train)))\n",
    "    print(\"{} Test score: {:.3f}\".format(name, m.score(X_test, y_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielhardt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg Training score: 0.999\n",
      "logreg Test score: 0.897\n",
      "naive bayes Training score: 0.924\n",
      "naive bayes Test score: 0.871\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(C=.1)\n",
    "applyModel(logreg,\"logreg\", X_train, y_train, X_test, y_test)\n",
    "nbModel = MultinomialNB()\n",
    "applyModel(nbModel,\"naive bayes\", X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "For both of the above models, sort the features by their coefficients, and print the top 10 features and their coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Lowest coefficients:\n",
      "('unfortunately', -0.541858013713471)\n",
      "('bad', -0.5464447135239142)\n",
      "('avoid', -0.5552750228561513)\n",
      "('annoying', -0.5691010503211021)\n",
      "('lame', -0.5871430788862058)\n",
      "('mess', -0.5915906804708063)\n",
      "('lacks', -0.5999421801520495)\n",
      "('the worst', -0.6050191333668404)\n",
      "('dull', -0.6101205556935515)\n",
      "('horrible', -0.6402199135044385)\n",
      "('worse', -0.6412519970608491)\n",
      "('terrible', -0.6416185026508155)\n",
      "('disappointing', -0.679593345185616)\n",
      "('poor', -0.6809414852586397)\n",
      "('disappointment', -0.7376499566778868)\n",
      "('poorly', -0.7459827842801316)\n",
      "('waste', -0.8597800622899489)\n",
      "('boring', -0.8789592423906969)\n",
      "('worst', -0.9532294508642182)\n",
      "('awful', -0.9539919970848368)\n",
      "Highest coefficients:\n",
      "('excellent', 0.7647372780175421)\n",
      "('perfect', 0.7124699796130035)\n",
      "('wonderful', 0.6594738585259878)\n",
      "('superb', 0.5847949349929705)\n",
      "('amazing', 0.5633187250922497)\n",
      "('enjoyable', 0.5616770687000522)\n",
      "('great', 0.5217275899987744)\n",
      "('today', 0.503113552150552)\n",
      "('brilliant', 0.4957537061751291)\n",
      "('incredible', 0.4795288611348073)\n",
      "('fun', 0.4618137577307044)\n",
      "('rare', 0.4535900552497017)\n",
      "('must see', 0.4473084299910051)\n",
      "('wonderfully', 0.4421185562522368)\n",
      "('refreshing', 0.42585323643001355)\n",
      "('favorite', 0.42346681132621433)\n",
      "('simple', 0.4208777872038691)\n",
      "('loved', 0.4206305397342689)\n",
      "('well worth', 0.4198038707655278)\n",
      "('fantastic', 0.4160386287055767)\n",
      "Multinomial NB\n",
      "Lowest coefficients:\n",
      "('worst movies have', -15.230726382346033)\n",
      "('worst movies of', -15.230726382346033)\n",
      "('worst movies ve', -15.230726382346033)\n",
      "('worst of them', -15.230726382346033)\n",
      "('worst performance', -15.230726382346033)\n",
      "('worst piece', -15.230726382346033)\n",
      "('worst piece of', -15.230726382346033)\n",
      "('worst show', -15.230726382346033)\n",
      "('would explain', -15.230726382346033)\n",
      "('would need to', -15.230726382346033)\n",
      "('wuhrer', -15.230726382346033)\n",
      "('yell at', -15.230726382346033)\n",
      "('you waste', -15.230726382346033)\n",
      "('your life watching', -15.230726382346033)\n",
      "('your time or', -15.230726382346033)\n",
      "('your time with', -15.230726382346033)\n",
      "('zero out', -15.230726382346033)\n",
      "('zero out of', -15.230726382346033)\n",
      "('zero stars', -15.230726382346033)\n",
      "('zombies is', -15.230726382346033)\n",
      "Highest coefficients:\n",
      "('his', -5.476377092813626)\n",
      "('he', -5.532603859772472)\n",
      "('by', -5.838064453575896)\n",
      "('who', -5.883497758307728)\n",
      "('an', -5.898256877136786)\n",
      "('from', -5.946299467017152)\n",
      "('her', -5.973788724993067)\n",
      "('they', -6.030739422449342)\n",
      "('has', -6.105072818537044)\n",
      "('so', -6.109217224076465)\n",
      "('like', -6.1212013156086655)\n",
      "('about', -6.204909990719004)\n",
      "('very', -6.205992958155335)\n",
      "('out', -6.226795362692895)\n",
      "('there', -6.235561392034883)\n",
      "('she', -6.271029235386644)\n",
      "('what', -6.27644022514132)\n",
      "('or', -6.277215619338866)\n",
      "('good', -6.278509280580087)\n",
      "('more', -6.304740292728956)\n"
     ]
    }
   ],
   "source": [
    "coefs=logreg.coef_[0]\n",
    "sorted_coefs = sorted ((zip(vect.get_feature_names(), coefs)),\n",
    "                key = lambda e:e[1], reverse=True)\n",
    "\n",
    "low = sorted_coefs[-20:]\n",
    "high = sorted_coefs[:20]\n",
    "print(\"Logistic Regression\")\n",
    "print(\"Lowest coefficients:\")\n",
    "for i in low:\n",
    "    print(i)\n",
    "print(\"Highest coefficients:\")\n",
    "for i in high:\n",
    "    print(i)\n",
    "    \n",
    "coefs=nbModel.coef_[0]\n",
    "sorted_coefs = sorted ((zip(vect.get_feature_names(), coefs)),\n",
    "                key = lambda e:e[1], reverse=True)\n",
    "\n",
    "low = sorted_coefs[-20:]\n",
    "high = sorted_coefs[:20]\n",
    "print (\"Multinomial NB\")\n",
    "print(\"Lowest coefficients:\")\n",
    "for i in low:\n",
    "    print(i)\n",
    "print(\"Highest coefficients:\")\n",
    "for i in high:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
